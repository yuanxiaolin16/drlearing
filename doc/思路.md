1.critic.net, critic target net, actor net, actor target net
2.replay buffer 经验池
3.确定性算法，在动作中加上随机量，使其探索更多的可能
4.for episode in M:
    1.reset初始化状态
    2.for step in episode
        1.get actor from state, + random N从状态量取计算动作
        2.actor -> next states根据动作量去计算下一步的状态
        3.replay buffer push存储在经验池中
        4.target_Q = target_next_Q(state)*gamma*(1-done) + reward
        5.loss(target_Q, critic net)算出来的奖励，与网络估计的奖励进行比较，计算loss
        6.update critic net根据loss值去更新critic网络
        7.update actor net根据critic网络值去更新actor网络
        8.update target net把新的网络平滑地赋值给之前存的网络，相当于在最新的网络和之前存档的网络之间用一个滤波器
